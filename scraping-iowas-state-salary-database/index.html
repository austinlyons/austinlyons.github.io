<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />

    <title>Scraping Iowa&#x27;s State Salary Database</title>

    <meta name="HandheldFriendly" content="True">
    <meta name="MobileOptimized" content="320">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

    <meta name="description" content="">

    <meta name="twitter:card" content="summary">
    <meta name="twitter:title" content="Scraping Iowa&#x27;s State Salary Database">
    <meta name="twitter:description" content="">

    <meta property="og:type" content="article">
    <meta property="og:title" content="Scraping Iowa&#x27;s State Salary Database">
    <meta property="og:description" content="">

    <link href="../favicon.ico" rel="shortcut icon" type="image/x-icon">
    <link href="http://127.0.0.1:2368/apple-touch-icon-precomposed.png" rel="apple-touch-icon">

    <link rel="stylesheet" type="text/css" href="../assets/css/uno.css?v=731201513b" />
    <link rel="stylesheet" type="text/css" href="../assets/css/custom.css?v=731201513b" />

    <link rel="canonical" href="http://www.seeaustinhack.com/scraping-iowas-state-salary-database/" />
    
    <meta property="og:site_name" content="AUSTIN LYONS" />
    <meta property="og:type" content="article" />
    <meta property="og:title" content="Scraping Iowa&#x27;s State Salary Database" />
    <meta property="og:description" content="In this post I describe how to scrape all 443,600 Iowa public salary  records from the Des Moines Register&#x27;s website by writing a simple Python program. Oooh, salary information! I&#x27;m trying to learn how to gain insights from data,..." />
    <meta property="og:url" content="http://www.seeaustinhack.com/scraping-iowas-state-salary-database/" />
    <meta property="article:published_time" content="2014-05-01T17:01:00.000Z" />
    <meta property="article:modified_time" content="2014-06-17T23:04:46.649Z" />
    <meta property="article:tag" content="archived" />
    <meta property="article:tag" content="python" />
    <meta property="article:tag" content="scraping" />
    <meta property="article:tag" content="scrapy" />
    <meta property="article:tag" content="tech" />
    
    <meta name="twitter:card" content="summary" />
    <meta name="twitter:title" content="Scraping Iowa&#x27;s State Salary Database" />
    <meta name="twitter:description" content="In this post I describe how to scrape all 443,600 Iowa public salary  records from the Des Moines Register&#x27;s website by writing a simple Python program. Oooh, salary information! I&#x27;m trying to learn how to gain insights from data,..." />
    <meta name="twitter:url" content="http://www.seeaustinhack.com/scraping-iowas-state-salary-database/" />
    
    <script type="application/ld+json">
{
    "@context": "http://schema.org",
    "@type": "Article",
    "publisher": "AUSTIN LYONS",
    "author": {
        "@type": "Person",
        "name": "Austin Lyons",
        "image": "//www.gravatar.com/avatar/aed0bec360cc1029f8cb094c7cdb11e7?d=404",
        "url": "http://www.seeaustinhack.com/author/austin-lyons",
        "sameAs": "http://www.seeaustinhack.com",
        "description": null
    },
    "headline": "Scraping Iowa&#x27;s State Salary Database",
    "url": "http://www.seeaustinhack.com/scraping-iowas-state-salary-database/",
    "datePublished": "2014-05-01T17:01:00.000Z",
    "dateModified": "2014-06-17T23:04:46.649Z",
    "keywords": "archived, python, scraping, scrapy, tech",
    "description": "In this post I describe how to scrape all 443,600 Iowa public salary  records from the Des Moines Register&#x27;s website by writing a simple Python program. Oooh, salary information! I&#x27;m trying to learn how to gain insights from data,..."
}
    </script>

    <meta name="generator" content="Ghost 0.6" />
    <link rel="alternate" type="application/rss+xml" title="AUSTIN LYONS" href="http://www.seeaustinhack.com/rss/" />

</head>
<body class="post-template tag-archived tag-python tag-scraping tag-scrapy tag-tech no-js">

    <span class="mobile btn-mobile-menu">
        <i class="icon icon-list btn-mobile-menu__icon"></i>
        <i class="icon icon-x-circle btn-mobile-close__icon hidden"></i>
    </span>

    <header class="panel-cover panel-cover--collapsed" style="background-image: url(../content/images/2014/Oct/may_the_sun_be_upon_your_face_by_fleurss.jpeg)">
  <div class="panel-main">

    <div class="panel-main__inner panel-inverted">
    <div class="panel-main__content">

        <h1 class="panel-cover__title panel-title"><a href="http://www.seeaustinhack.com" title="link to homepage for AUSTIN LYONS">AUSTIN LYONS</a></h1>
        <span class="panel-cover__subtitle panel-subtitle">Software Engineer</span>
        <hr class="panel-cover__divider" />
        <p class="panel-cover__description">
          <p>I am not the man I ought to be, I am not the man I wish to be, and I am not the man I hope to be, but by the grace of God, I am not the man I used to be.
          <br>
          John Newton <a href="http://books.google.com/books?id=mv4oAAAAYAAJ&dq=ah%2C%20how%20imperfect%20and%20deficient!%20I%20am%20not%20what%20I%20wish%20to%20be&pg=PA186#v=onepage&q=ah%2C%20how%20imperfect%20and%20deficient!%20I%20am%20not%20what%20I%20wish%20to%20be&f=false" target="_blank">(paraphrased)</a>
        </p>
        <hr class="panel-cover__divider panel-cover__divider--secondary" />

        <div class="navigation-wrapper">

          <nav class="cover-navigation cover-navigation--primary">
            <ul class="navigation">
              <li class="navigation__item"><a href="../index.html#blog" title="link to AUSTIN LYONS blog" class="blog-button">Technical Blog</a></li>
              <li class="navigation__item"><a href="https://medium.com/@theaustinlyons/" class="blog-button" window="_blank">Product & Strategy Blog</a></li>
              <li class="navigation__item"><a href="../reading-list/index.html" title="link to reading list" class="reading-button">Currently Reading</a></li>
            </ul>
          </nav>

          
<nav class="cover-navigation navigation--social">
  <ul class="navigation">
    <!-- GitHub -->
    <li class="navigation__item">
      <a href="https://github.com/austinlyons" title="Austin Lyons on GitHub">
        <i class='icon icon-social-github'></i>
        <span class="label">GitHub</span>
      </a>
    </li>

    <!-- LinkedIn -->
    <li class="navigation__item">
      <a href="https://www.linkedin.com/in/austinlyons" title="Austin Lyons on LinkedIn">
        <i class='icon icon-social-linkedin'></i>
        <span class="label">LinkedIn</span>
      </a>
    </li>


  <!-- Twitter -->
  <li class="navigation__item">
    <a href="http://twitter.com/theaustinlyons" title="@theaustinlyons on Twitter">
      <i class='icon icon-social-twitter'></i>
      <span class="label">Twitter</span>
    </a>
  </li>

  </ul>
</nav>

        </div>

      </div>

    </div>

    <div class="panel-cover--overlay cover-slate"></div>
  </div>
</header>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
        m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
          })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

            ga('create', 'UA-36006810-1', 'auto');
              ga('send', 'pageview');

</script>

    <div class="content-wrapper">
        <div class="content-wrapper__inner">
<!--          <div id="top-nav">
            <ul>
              <li><a href="/">Home</li>
              <li><a href="/tag/tech/">Tech</li>
              <li><a href="/tag/startups/">Startups</li>
              <li><a href="/tag/faith/">Faith</a></li>
              <li><a href="/tag/life/">Life</a></li>
            </ul>
          </div> -->
            

  <article class="post-container post-container--single">

    <header class="post-header">
      <div class="post-meta">
        <time datetime="01 May 2014" class="post-meta__date date">01 May 2014</time> &#8226; <span class="post-meta__tags tags">on <a href="../tag/archived/index.html">archived</a>, <a href="../tag/python/index.html">python</a>, <a href="../tag/scraping/index.html">scraping</a>, <a href="../tag/scrapy/index.html">scrapy</a>, <a href="../tag/tech/index.html">tech</a></span>
        <!--<span class="post-meta__author author"><img src="//www.gravatar.com/avatar/aed0bec360cc1029f8cb094c7cdb11e7?d=404" alt="profile image for Austin Lyons" class="avatar post-meta__avatar" /> by Austin Lyons</span>-->
      </div>
      <h1 class="post-title">Scraping Iowa&#x27;s State Salary Database</h1>
    </header>

    <section class="post tag-archived tag-python tag-scraping tag-scrapy tag-tech">
      <p>In this post I describe how to scrape all 443,600 Iowa public salary <br />
records from the Des Moines Register's website by writing a simple Python program.</p>

<h4 id="ooohsalaryinformation">Oooh, salary information!</h4>

<p>I'm trying to learn how to gain insights from data, so I need a dataset <br />
to play with as I learn. (I learn best from actually doing, not just <br />
reading). Because I'm from Iowa, I know of a nice dataset that will be <br />
very interesting to play around with - a list of the salaries for all <br />
state employees. The Des Moines Register provides a web-interface to <br />
this data at <br />
<a href="http://data.desmoinesregister.com/dmr/dmr-public-records/state_salaries.php">http://data.desmoinesregister.com/dmr/dmr-public-records/state<em>salaries.php</a>.
_Thank you Des Moines Register for providing this data via the web</em>.</p>

<p>While the web-interface is great for some simple searching, it only returns up to 100 results at a time. I will need <em>all of the data</em>, preferably in a single file, probably comma-separated (.csv) or formatted as <a href="http://www.w3schools.com/json/">JSON</a>. I might be able to call or email someone at the <a href="http://www.desmoinesregister.com">Des Moines Register</a> and see if they can share the data with me, but it would likely take a while to hear back from anyone. Even if they do respond right away, because there are so many entries (443610), the dataset will be too big to email so we'd have to figure out another way for me to receive it.  </p>

<p>If you want to learn how to write a scraper to obtain the data, keep <br />
reading. If you're ready for the the data <br />
(CSV and JSON) or want to take a peek at the scraper source code, head on over to my <a href="https://github.com/austinlyons/iowa_salaries">GitHub
repo</a>.</p>

<h4 id="programmingftw">Programming FTW</h4>

<p>Doesn't it sound more fun to write a computer program that can <br />
(responsibly) visit the Des Moines Register's state salary website, find the relevant information, and
store it for us? We'd have all the data in a single file and would <br />
learn a bit about crawling the web. And anyway, we don't possibly have <br />
the time or access to freshman interns to visit all 4360 pages of search results <br />
and copy the data manually. (Or as Hilary Mason puts it, <a href="http://www.dataists.com/2010/09/a-taxonomy-of-data-science/">pointing and <br />
clicking does not <br />
scale</a>).</p>

<p>Writing a web crawler to obtain interesting data seems like something we'd do again in the future anyway, so let's take the time to learn and build one now.</p>

<h4 id="python">Python</h4>

<p>When I want to write a short program to do useful work on my behalf, <a href="http://www.python.org">Python</a> is my go-to programming language. It's easy to use and well documented. <br />
If you've never learned Python before, do yourself a favor and head on <br />
over to <a href="https://www.udacity.com/course/cs101">Udacity's free CS 101 <br />
course</a>. All you need is a Saturday <br />
morning or two at Starbucks to watch the videos and you'll know enough to be dangerous. (Or at least enough to know what to Google so that you successfully find the <a href="http://stackoverflow.com/">StackOverflow</a> answer to your question). Also check out Kushal Das' <a href="http://pymbook.readthedocs.org/en/latest/index.html">Python for You and Me</a> or Zed Shaw's <a href="http://learnpythonthehardway.org/book/">Learn Python the Hard Way</a>. </p>

<p>If you don't have Python yet, now is a good time to <a href="http://www.python.org/getit/">install <br />
it</a>.</p>

<p>In theory, now is a good time for me to tell you what virtual <br />
environments are in case you're new to Python.  Instead, I'll just tell <br />
you that if you find yourself regularly writing Python programs, <br />
check out <a href="index.html">virtualenv</a>. </p>

<h4 id="scrapingwithscrapy">Scraping with Scrapy</h4>

<p>Websites are mostly just files full of <a href="http://www.w3schools.com/html/">nicely structured <br />
HTML</a>, and <br />
structure is the best friend of someone writing a computer program to find or discover <br />
information. Many, many businesses and individuals write programs to crawl and scrape <br />
information from websites, and some of those people are nice enough to <br />
share their code with the rest of us to make our life easier. One particular free, open-source <br />
library named <a href="http://doc.scrapy.org/en/0.18/">Scrapy</a> makes crawling the web with Python simple. <br />
(Thank you Scrapy committers!)</p>

<h4 id="scrapyoverview">Scrapy Overview</h4>

<p>First, let's read the <a href="http://doc.scrapy.org/en/0.18/intro/overview.html">Scrapy overview</a> to see an <br />
example and learn a bit more. Wow - the entire example program on that page is 14 <br />
lines of code, can be run with a simple command from the command line, and can automatically export the data to a JSON file. PERFECT!</p>

<p>Not that familiar with XPath? Me neither. It looks pretty straightforward though.</p>

<h4 id="installingscrapy">Installing Scrapy</h4>

<p>By this point you should have Python installed. You should also install <br />
pip if you don't have it yet; <a href="http://www.pip-installer.org/en/latest/installing.html">pip</a> is a Python package manager that will <br />
help you securely download and install Python packages.</p>

<p>Next, <a href="http://doc.scrapy.org/en/0.18/intro/install.html">install Scrapy</a></p>

<p>(I’ll use the dollar sign ($) to represent the terminal prompt from here on out, so you’ll
type everything after the dollar sign).</p>

<pre><code>$ pip install Scrapy
</code></pre>

<p>If everything works you should be able to type</p>

<pre><code>$ which scrapy
</code></pre>

<p>and get a path to the Scrapy installation. You could also do</p>

<pre><code>$ scrapy version
</code></pre>

<p>and you should see a particular Scrapy version number. When I do this I <br />
see <code>Scrapy 0.16.4</code></p>

<h4 id="creatingascrapyproject">Creating a Scrapy project</h4>

<p>Let's create a scrapy project and call it iowa_salaries. </p>

<pre><code>$ scrapy startproject iowa_salaries
</code></pre>

<p>You will have a folder called iowa_salaries. Enter into the folder and <br />
look at it's contents.</p>

<pre><code>$ cd iowa_salaries
$ ls
</code></pre>

<p>You should see a spiders folder and some python files.</p>

<pre><code>__init__.py
items.py
pipelines.py
settings.py
spiders
</code></pre>

<h4 id="projectsettings">Project Settings</h4>

<p>Add this to your settings.py file. Update the user agent information to identify <br />
yourself. Note that we're telling Scrapy to <a href="http://doc.scrapy.org/en/latest/topics/settings.html#download-delay">pause between requests</a> <br />
so that we don't overwhelm the Des Moines Register's servers. Always do this. <br />
We aren't being malicious, we are just requesting their websites in an <br />
automated fashion.</p>

<pre><code># Scrapy settings for iowa_salaries project
#
# For simplicity, this file contains only the most important settings by
# default. All the other settings are documented here:
#
#     http://doc.scrapy.org/topics/settings.html
#


# To export to json: scrapy crawl iowa_salaries --set FEED_URI=salaries.json --set FEED_FORMAT=json
# To export to a CSV: scrapy crawl iowa_salaries --set FEED_URI=salaries.csv --set FEED_FORMAT=csv
BOT_NAME = 'iowa_salaries'

SPIDER_MODULES = ['iowa_salaries.spiders']
NEWSPIDER_MODULE = 'iowa_salaries.spiders'

# Crawl responsibly by identifying yourself (and your website) on the user-agent
USER_AGENT = 'iowa_salaries (+http://www.yoururlhere.com)'

# Crawl responsibly by adding some delay
DOWNLOAD_DELAY = 0.25
RANDOMIZE_DOWNLOAD_DELAY = True
</code></pre>

<h4 id="datastructure">Data Structure</h4>

<p>Let's define the data structure that will hold the scraped data. We'll edit items.py. Create a class called <code>IowaSalaryItem</code> to hold the scraped <br />
data. What data should our IowaSalaryItem contain? The <a href="http://data.desmoinesregister.com/dmr/dmr-public-records/state_salaries.php">DMR salary <br />
database</a> has several pieces of information for each entry: name of the employee, salary, year, department, position, county, sex, base salary ("july salary"), and extra money (travel, etc). We'll just save it all - it's easier to filter out the information that we don't want than to go back and scrape it all again (responsibly scraping a big dataset might take an hour). </p>

<p>Edit your items.py file as follows:</p>

<pre><code># Define the models for your scraped items here
# See the scrapy documentation for more information
# http://doc.scrapy.org/topics/items.html

from scrapy.item import Item, Field

class IowaSalaryItem(Item):  
    year = Field()
    salary = Field()
    employee = Field()
    department = Field()
    position = Field()
    county = Field()
    sex = Field()
    base_salary = Field()
    extra_money = Field()
</code></pre>

<h4 id="spider">Spider</h4>

<p>Now we'll write a spider to crawl the web. As demonstrated on the <a href="http://doc.scrapy.org/en/0.18/intro/tutorial.html#our-first-spider">Scrapy <br />
tutorial</a>, we need to tell the spider which webpages to download and what to do with the pages once we've downloaded them. In our case we want to download every webpage from the Des Moines Register that has salary information on it, and then search through every downloaded page and find and store each mention of employee, salary, department, year, etc. </p>

<p>Create a file in <code>spiders</code> called <code>iowa_salaries_spider.py</code>. Add the <br />
following code and save the file.</p>

<pre><code>from scrapy.spider import BaseSpider  
from scrapy.selector import HtmlXPathSelector  
from iowa_salaries.items import IowaSalaryItem

def _select_data(r, xpath):  
    '''Return the first element if it exists, else return None'''
    data = r.select(xpath).extract()
    return data[0] if data else None

class iowa_salaries_spider(BaseSpider):  
    name = "iowa_salaries"
    allowed_domains = ["desmoinesregister.com"]
    start_urls = \
      ["http://data.desmoinesregister.com/dmr/dmr-public-records/state_salaries.php?BRSR=%s" 
        % i for i in range(0, 443700, 100)]

    def parse(self, response):
        hxs = HtmlXPathSelector(response)
        items = []
        rows = hxs.select("//tr[@class='tr0' or @class='tr1']")
        for r in rows:
            year = _select_data(r, "td[1]/a/text()")
            salary = _select_data(r, "td[2]/text()")
            employee = _select_data(r, "td[3]/a/text()")
            department = _select_data(r, "td[4]/a/text()")
            position = _select_data(r, "td[5]/text()")
            county = _select_data(r, "td[6]/text()")
            sex = _select_data(r, "td[7]/text()")
            base_salary = _select_data(r, "td[8]/text()")
            extra_money = _select_data(r, "td[9]/text()")

            if employee:
                item = IowaSalaryItem()
                item['year'] = year
                item['salary'] = salary
                item['employee'] = employee
                item['department'] = department
                item['position'] = position
                item['county'] = county
                item['sex'] = sex
                item['base_salary'] = base_salary
                item['extra_money'] = extra_money
                items.append(item)

        return items
</code></pre>

<h4 id="urlstocrawl">URLs to crawl</h4>

<p>A Scrapy spider can download and parse a single webpage or it can be configured to visit a list <br />
of URLs. The true power of a Scrapy spider is it's ability to easily visit links <br />
that it <em>discovers</em>. That is, you can give a Scrapy spider a list of websites to download and tell it to also download and parse any websites that are linked to by the initial websites. Pretty cool!</p>

<p>In this case, we don't really need to discover which URLs to visit. If <br />
we investigate a bit we'll see a pattern in the URLs that return salary <br />
information. In particular, check out <br />
<a href="http://data.desmoinesregister.com/dmr/dmr-public-records/state_salaries.php">http://data.desmoinesregister.com/dmr/dmr-public-records/state<em>salaries.php</a>.
If we click to go to page 2, we're taken to the URL <br />
<a href="http://data.desmoinesregister.com/dmr/dmr-public-records/state_salaries.php?BRSR=100">http://data.desmoinesregister.com/dmr/dmr-public-records/state</em>salaries.php?BRSR=100</a>.
Now click to visit page 3 and you'll be taken to <br />
<a href="http://data.desmoinesregister.com/dmr/dmr-public-records/state_salaries.php?BRSR=200">http://data.desmoinesregister.com/dmr/dmr-public-records/state_salaries.php?BRSR=200</a>.</p>

<p>Notice the pattern? Each page has the same URL, except the value at the <br />
very end of the URL after <code>BRSR=</code> changes. <br />
Page 2 is addressed by <code>BRSR=100</code>, page 3 by <code>BRSR=200</code>, and we can <br />
correctly predict that page 4 will be the original URL with <code>BRSR=300</code> at the end. Do <br />
you think we can visit the first page with <code>BRSR=0</code> at the end? You're <br />
right. How convenient.</p>

<p>(By the way, what does BRSR stand for? I don't know. Maybe <a href="http://brsr.net/">Business
Records Storage and Retrieval</a>???)</p>

<p>Notice that each page has 100 entries on it, so the URL is <br />
essentially just requesting that the website return entries <code>n</code> to <br />
<code>n+99</code>. Page 2 will start with entry 100 and end with entry 199.
Therefore, all we really need to know is how many total entries there <br />
are, and then we'll get 100 entries at a time from each page until we've <br />
obtained all of the entries. Said another way, if there are 800 total <br />
entries, then we need to visit 8 URLs to obtain all of the data.</p>

<p>It turns out that there are 443610 entries in the Des Moines Register's <br />
salary database. So our web crawler will need to download and parse 4436 websites to get <em>all of the data</em>. <br />
See what I mean about not having enough freshman interns to do this <br />
manually! </p>

<p>Since we can just give Scrapy a list of URLs to visit, we just need to create a realllly long list of URLs:</p>

<pre><code>http://data.desmoinesregister.com/dmr/dmr-public-records/state_salaries.php?BRSR=0
http://data.desmoinesregister.com/dmr/dmr-public-records/state_salaries.php?BRSR=100
http://data.desmoinesregister.com/dmr/dmr-public-records/state_salaries.php?BRSR=200
http://data.desmoinesregister.com/dmr/dmr-public-records/state_salaries.php?BRSR=300
...
http://data.desmoinesregister.com/dmr/dmr-public-records/state_salaries.php?BRSR=443600
</code></pre>

<p>Fortunately, with Python we can make such a list with one line of code <br />
using <a href="http://docs.python.org/2/tutorial/datastructures.html#list-comprehensions">list <br />
comprehensions</a>:</p>

<pre><code>start_urls = \  
  ["http://data.desmoinesregister.com/dmr/dmr-public-records/state_salaries.php?BRSR=%s" 
    % i for i in range(0, 443700, 100)]
</code></pre>

<p><code>range(0, 100, 443700)</code> returns a list of integers that start with 0 and counts up to 443600 in increments of 100. i.e. <code>[0, 100, 200, ... 443600]</code> </p>

<h4 id="parsing">Parsing</h4>

<p>We know how to use Python to create a list of URLs for Scrapy to visit. <br />
But what should Scrapy do when it downloads the HTML returned by visiting each URL? <br />
Scrapy does whatever we tell it to do in the parse() method of our <br />
spider to do. In our case, we want to create a IowaSalaryItem and fill <br />
it's fields (salary, year, etc) with whatever values we find on the <br />
downloaded webpage.</p>

<h4 id="scrapyshell">Scrapy Shell</h4>

<p>We need a way to try out XPath selectors on the HTML we receive from the <br />
DMR. In particular, we want XPath selectors to extract the data in each <br />
row of the results table (year, employee, etc).</p>

<p>Fortunately, we have Scrapy shell! (see <a href="http://doc.scrapy.org/en/0.18/intro/tutorial.html#trying-selectors-in-the-shell">example <br />
here</a> and <a href="http://doc.scrapy.org/en/0.18/topics/shell.html#topics-shell">documentation <br />
here</a>). <br />
Note that you'll need to install <a href="http://ipython.org/">iPython</a> if you don't have it already.</p>

<p>Let's try it out. By now you should have created a scrapy project in a <br />
folder called <code>iowa_salaries</code> with an edited <code>settings.py</code>, <code>items.py</code>, and <br />
<code>spiders/iowa_salaries_spider.py</code>. Navigate to the root folder of the
project (<code>iowa_salaries</code>) and run:</p>

<pre><code>$ scrapy shell http://data.desmoinesregister.com/dmr/dmr-public-records/state_salaries.php
</code></pre>

<p>You should get a verbose response that has some information like:</p>

<pre><code>2013-09-04 18:28:39-0500 [scrapy] INFO: Scrapy 0.16.4 started (bot:
iowa_salaries)
...
2013-09-04 18:28:39-0500 [iowa_salaries] INFO: Spider opened
2013-09-04 18:28:40-0500 [iowa_salaries] DEBUG: Crawled (200) &lt;GET
http://data.desmoinesregister.com/dmr/dmr-public-records/state_salaries.php&gt;
(referer: None)
[s] Available Scrapy objects:
[s]   hxs        &lt;HtmlXPathSelector xpath=None data=u'&lt;html
lang="en"&gt;&lt;!-- COMO/GCI v1.18 Revi'&gt;
[s]   item       {}
[s]   request    &lt;GET
http://data.desmoinesregister.com/dmr/dmr-public-records/state_salaries.php&gt;
[s]   response   &lt;200
http://data.desmoinesregister.com/dmr/dmr-public-records/state_salaries.php&gt;
[s]   settings   &lt;CrawlerSettings module=&lt;module
'iowa_salaries.settings' 
[s]   spider     &lt;iowa_salaries_spider 'iowa_salaries' at 0x10a79e8d0&gt;
[s] Useful shortcuts:
[s]   shelp()           Shell help (print this help)
[s]   fetch(req_or_url) Fetch request (or URL) and update local objects
[s]   view(response)    View response in a browser
Python 2.7.1 (r271:86832, Aug  5 2011, 03:30:24)
Type "copyright", "credits" or "license" for more information.

IPython 0.13 -- An enhanced Interactive Python.
?         -&gt; Introduction and overview of IPython's features.
%quickref -&gt; Quick reference.
help      -&gt; Python's own help system.
object?   -&gt; Details about 'object', use 'object??' for extra details.
</code></pre>

<p>As you can see, Scrapy downloaded the webpage and gives us access to <br />
some Scrapy objects. We can view the request object, see all of the attributes of the <br />
request object, and print the details of a request object attribute.</p>

<pre><code>In [6]: print request
&lt;GET http://data.desmoinesregister.com/dmr/dmr-public-records/state_salaries.php&gt;

In [7]: print dir(request)
['__class__', '__delattr__', '__dict__', '__doc__', '__format__',
'__getattribute__', '__hash__', '__init__', '__module__', '__new__',
'__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__',
'__slots__', '__str__', '__subclasshook__', '__weakref__', '_body',
'_encoding', '_get_body', '_get_url', '_meta', '_set_body', '_set_url',
'_url', 'body', 'callback', 'cookies', 'copy', 'dont_filter',
'encoding', 'errback', 'headers', 'meta', 'method', 'priority',
'replace', 'url']

In [8]: print request.headers
{'Accept-Language': ['en'], 'Accept-Encoding': ['x-gzip,gzip,deflate'],
'Accept':
['text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8'],
'User-Agent': ['iowa_salaries (+http://www.yourdomainhere.com)']}
</code></pre>

<p>We can explore the response in a similar fashion.   </p>

<pre><code>In [14]: print response.meta
{'download_timeout': 180, 'handle_httpstatus_all': True,
'download_latency': 0.4923880100250244, 'depth': 0}
</code></pre>

<h4 id="htmlxpathselectorandthespidercode">HTMLXPath Selector and the spider code</h4>

<p>Lets play with the <code>hxs</code> object. Can we get each row of the results <br />
table?</p>

<pre><code>In [16]: hxs.select("//tr[@class='tr0' or @class='tr1']")
Out[16]:
[&lt;HtmlXPathSelector xpath="//tr[@class='tr0' or @class='tr1']"
data=u'&lt;tr class="tr0" valign="top"&gt;&lt;td style="'&gt;,
....
</code></pre>

<p>Let's check the contents to see if it worked.</p>

<pre><code>In [17]: hxs.select("//tr[@class='tr0' or @class='tr1']").extract()
Out[17]:
[u'&lt;tr class="tr0" valign="top"&gt;&lt;td style="width: 5%;"&gt;&lt;a
style="width: 8%;" align="right"&gt;$3,725,000.00&lt;/td&gt;&lt;td style="width:
20%;"&gt;&lt;a
href="state_salaries.php?orderby=EMPLOYEE_name&amp;amp;keyEMPLOYEE=FERENTZ%20KIRK%20J"&gt;FERENTZ
KIRK J&lt;/a&gt;&lt;/td&gt;&lt;td style="width: 20%;"&gt;&lt;a
href="state_salaries.php?orderby=DEPARTMENT_name&amp;amp;keyDEPARTMENT=UNIVERSITY%20OF%20IOWA"&gt;UNIVERSITY
OF IOWA&lt;/a&gt;&lt;/td&gt;&lt;td style="width: 18%;"&gt;HEAD COACH&lt;/td&gt;&lt;td style="width:
10%;"&gt;JOHNSON&lt;/td&gt;&lt;td style="width: 3%;"&gt;M&lt;/td&gt;&lt;td style="width: 8%;"
align="right"&gt;$1,770,000.00&lt;/td&gt;&lt;td style="width: 8%;"
align="right"&gt;$4,629.42&lt;/td&gt;&lt;/tr&gt;',
...
</code></pre>

<p>Excellent! It worked! Wondering where <code>hxs.select("//tr[@class='tr0' or @class='tr1']").extract()</code> came from? <br />
Open up <br />
<a href="http://data.desmoinesregister.com/dmr/dmr-public-records/state_salaries.php">http://data.desmoinesregister.com/dmr/dmr-public-records/state_salaries.php</a>
in Google Chrome, hover your mouse over <code>Kirk Ferentz</code>, right click, and <br />
choose <code>Inspect Element</code>. You'll see the HTML code, and in particular, <br />
you'll notice that Kirk Ferentz is in a row in the table with the class <br />
<code>resultstable</code>. Look around a bit and you'll see that every row is
labled <code>class='tr0'</code> or <code>class='tr1'</code>.  </p>

<h4 id="spidercode">Spider code</h4>

<p>Now scroll back up to iowa<em>salaries</em>spider.py and look at lines 20 and <br />
21.</p>

<pre><code>rows = hxs.select("//tr[@class='tr0' or @class='tr1']")
for r in rows:
</code></pre>

<p>You'll see that we use this xpath to get all of the rows of the results <br />
table. Then we loop through each row and select the text from each column. </p>

<pre><code>for r in rows:  
        year = _select_data(r, "td[1]/a/text()")
        salary = _select_data(r, "td[2]/text()")
        employee = _select_data(r, "td[3]/a/text()")
        department = _select_data(r, "td[4]/a/text()")
        position = _select_data(r, "td[5]/text()")
        county = _select_data(r, "td[6]/text()")
        sex = _select_data(r, "td[7]/text()")
        base_salary = _select_data(r, "td[8]/text()")
        extra_money = _select_data(r, "td[9]/text()")
</code></pre>

<p>You'll notice I wrote a little helper function that tries to select the <br />
data with the given xpath, and if it doesn't exist it returns None.</p>

<pre><code>def _select_data(r, xpath):  
    '''Return the first element if it exists, else return None'''
    data = r.select(xpath).extract()
    return data[0] if data else None
</code></pre>

<p>Finally we check to make sure at least the employee name exists (sanity <br />
check), and if so we store all of the data in the IowaSalaryItem and <br />
append it to the list of items we've processed.</p>

<pre><code>if employee:  
    item = IowaSalaryItem()
    item['year'] = year
    item['salary'] = salary
    item['employee'] = employee
    item['department'] = department
    item['position'] = position
    item['county'] = county
    item['sex'] = sex
    item['base_salary'] = base_salary
    item['extra_money'] = extra_money
    items.append(item)
</code></pre>

<h4 id="themomentoftruth">The Moment of Truth</h4>

<p>Let's crawl! But first, edit the spider so that it only crawls one page. <br />
Change <code>range(0, 443700, 100)</code> to <code>range(0, 100, 100)</code>. We'll crawl a <br />
single page to make sure it worked and check that the output looks like <br />
we expect.</p>

<p>Now navigate to the top level directory. Let's run the crawler and <br />
export all of the IowaSalaryItem that we scrape to a JSON file.</p>

<pre><code>scrapy crawl iowa_salaries --set FEED_URI=salaries.json --set FEED_FORMAT=json
</code></pre>

<p>If it worked, you should see a lot of interesting output from Scrapy as <br />
it's crawling in your terminal. You'll also have a <code>salaries.json</code> file <br />
in your directory. Let's look at the first two lines of the file.</p>

<pre><code>$ head -2 salaries.json
[{"salary": "$3,725,000.00", "extra_money": "$4,629.42", "base_salary":
"$1,770,000.00", "sex": "M", "county": "JOHNSON", "department":
"UNIVERSITY OF IOWA", "year": "2012", "employee": "FERENTZ KIRK J",
"position": "HEAD COACH"},
{"salary": "$1,425,000.00", "extra_money": "$8,907.66", "base_salary":
"$375,000.00", "sex": "M", "county": "STORY", "department": "IOWA STATE
UNIVERSITY", "year": "2012", "employee": "RHOADS PAUL R", "position":
"HEAD COACH"},
</code></pre>

<p>Excellent, it worked. If you look at the entire file, it will have all <br />
100 entries. You can verify that it's valid JSON (and see it nicely <br />
formatted) by pasting the contents of the file into <br />
<a href="http://127.0.0.1:2368/scraping-iowas-state-salary-database/jsonlint.com">jsonlint.com</a>. </p>

<p>(Never used the <code>head</code> Unix utility before? Check out <a href="http://www.gregreda.com/2013/07/15/unix-commands-for-data-science/">Unix Commands for
Data <br />
Science</a> <br />
from Greg Rada for more tips &amp; tricks.)</p>

<h4 id="allofthedata">All of the data</h4>

<p>Time to obtain all of the data. Change your spider to crawl all of the <br />
pages and then run scrapy again:</p>

<pre><code>scrapy crawl iowa_salaries --set FEED_URI=salaries.json --set FEED_FORMAT=json
</code></pre>

<p>Twenty minutes or so later, you'll have a 97 MB <code>salaries.json</code> file. If you <br />
would rather export directly to a CSV, go for it:</p>

<pre><code>scrapy crawl iowa_salaries --set FEED_URI=salaries.csv --set FEED_FORMAT=csv
</code></pre>

<p>In another twenty minutes or so, you'll have a 43 MB <code>salaries.csv</code> file.</p>

<h4 id="summary">Summary</h4>

<p>In conclusion, we can use Scrapy to crawl the Des Moines Register and <br />
extract 443610 public salary records.</p>

<p>If you'd like to see the source code or download the resulting dataset <br />
as a CSV or JSON, head on over to <a href="https://github.com/austinlyons/iowa_salaries">my GitHub <br />
repo</a>.</p>

<p>If you've made it this far and have any feedback or just want to say hi, <br />
<a href="https://twitter.com/theaustinlyons">hit me up on Twitter</a>.</p>
    </section>

  </article>




            <footer class="footer">
    <span class="footer__copyright">&copy; 2015. All rights reserved.</span>
</footer>
        </div>
    </div>

    <script src="../public/jquery.js?v=731201513b"></script>

    <script type="text/javascript" src="../assets/js/main.js?v=731201513b"></script>

</body>
</html>
